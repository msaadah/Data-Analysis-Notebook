{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **STEP 1: Importing packages and Reading Data**    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **IMPORTING PACKAGES**\n",
    "\n",
    "**Below is the steps we would go through to load, view and visualize any xml data into a pandas dataframe.**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as et   # required to extract data from xml format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We add python packages we require.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy  as np                   # easy to play with arrays etc.\n",
    "import pandas as pd                   # required to load and read data and put in dataframe.\n",
    "import matplotlib.pyplot as plt       # required for data visualization purposes.\n",
    "import seaborn as sns                 # required for data visualization purposes.\n",
    "import plotly.plotly as py            # required for data visualization purposes.\n",
    "import plotly.graph_objs as go        # required for data visualization purposes.\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **READING DATA**  \n",
    "\n",
    "**To read data in the form of .xml, you need et.parse()**  \n",
    "**We also need to get the root for this we use .getroot()**\n",
    "\n",
    "**XML files are all strcutured differently, so you need to explore a bit**  \n",
    "\n",
    "**https://catalog.data.gov/dataset/age-adjusted-death-rates-for-the-top-10-leading-causes-of-death-united-states-2013 is source of data**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c77c9b63ed2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ntriples\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#as name for parser you can use ntriples, turtle, rdfxml, ...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_into_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"file://file_path\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtriple\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"triple.subject, triple.predicate, triple.objectparsedXML = et.parse('rows.xml')\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RDF' is not defined"
     ]
    }
   ],
   "source": [
    "parser=RDF.Parser(name=\"ntriples\") #as name for parser you can use ntriples, turtle, rdfxml, ...\n",
    "model=RDF.Model()\n",
    "stream=parser.parse_into_model(model,\"file://file_path\")\n",
    "for triple in model:\n",
    "    print(\"triple.subject, triple.predicate, triple.objectparsedXML = et.parse('rows.xml')\")\n",
    "root = parsedXML.getroot()\n",
    "\n",
    "dictionary_to_df = {}\n",
    "\n",
    "for neighbor in root.findall(\"row/row\"):\n",
    "    #print neighbor.tag,neighbor.attrib\n",
    "    for child in neighbor:\n",
    "        #The below two lines should be added to make sure all the arrays in the dictionary stay the same length.\n",
    "        #If you remove those lines and run, make sure to run the last two lines in this cell to see the sizes of arrays.\n",
    "        if child.tag == 'predicted_value' or child.tag == 'footnote':\n",
    "            continue\n",
    "            \n",
    "        if(child.tag not in dictionary_to_df):\n",
    "            dictionary_to_df[child.tag]=[child.text] \n",
    "        else:            \n",
    "            dictionary_to_df[child.tag].append(child.text)\n",
    "        \n",
    "# for key, array in  dictionary_to_df.items():\n",
    "    # print key, len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 3: PUTTING THE DATA IN A PANDAS DATAFRAME**  \n",
    "  \n",
    "\n",
    "**We use pd.DataFrame() to put the data we extracted into a pandas df format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame.from_dict(dictionary_to_df)\n",
    "display(HTML(data.head().to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 4: DECIDING WHICH COLUMNS MATTER**  \n",
    "  \n",
    "\n",
    "**There are several columns with data that is redundant. Essentially we pick the columns we want, in the order that we want and discard the rest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['state', 'year', 'month', 'percent_complete', 'percent_pending_investigation','data_value']]\n",
    "print (\"\\t\\t\\t The Final Dataset\")\n",
    "display(HTML(data.head().to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 5: VIEWING THE FIRST FEW ROWS**  \n",
    "  \n",
    "\n",
    "**To see the first few rows of the data and make sure we read it in correctly, we use .head()**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(data.head(15).to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 6: GET BASIC INFORMATION**  \n",
    "  \n",
    "**To get basic info from the dataset, we use .info()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 7: SEE FURTHER DETAILS**  \n",
    "  \n",
    "**To get datatypes of each column, we can use .dtypes**  \n",
    "\n",
    "**To get more details about each column, we can use .describe()**  \n",
    "  \n",
    "The reason we only get data from 3 columns is because the rest have commas in them which need to be removed\n",
    "We can deal with this later while cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (data.dtypes)\n",
    "print ()\"\\n\")\n",
    "display(HTML(data.describe().to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 8: COUNT NUMBER OF EMPTY VALUES IN COLUMN**  \n",
    "   \n",
    "**We can check the number of null values a column has by using .isnull().sum()**  \n",
    "  \n",
    "**For example, here, Climate has the most null values **  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 9: SEE NUMBER OF UNIQUE VALUES IN COLUMN**  \n",
    "  \n",
    "**It is useful to see the number of unique values in each column using .nunique()**  \n",
    "  \n",
    "**Here we see state year and month have a good number of unique values to order by, therefore we can group by these columns and make good visualizations**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (data.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 10: NUMBER OF OCCURANCES OF EACH VALUE IN COLUMN**  \n",
    "  \n",
    "**A good way to visualize data of a column you wish to group by is to use .value_counts()**  \n",
    "  \n",
    "**It gives a clear picture of how many would be in each group etc.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = data.year.value_counts()\n",
    "print (year)\n",
    "print (\"\\n\")\n",
    "\n",
    "month = data.month.value_counts()\n",
    "print (month)\n",
    "print ()\"\\n\")\n",
    "\n",
    "state = data.state.value_counts()\n",
    "print (state.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 10: CLEANING DATA**  \n",
    "  \n",
    "**To see more from the data it has to be cleaned. Cleaning data is usally unique to each dataset.**  \n",
    "\n",
    "**In this instance, we change column dtypes **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.columns:\n",
    "    if i== 'state' or i=='year' or i=='month':\n",
    "        \n",
    "        data[i] = data[i].str.strip().astype('category')\n",
    "    else:      \n",
    "        data[i] = data[i].str.strip('+-').astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now after cleaning, lets look at the new data types and the mean, std, min, max etc. of all the columns again**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (data.dtypes)\n",
    "print ()\"\\n\")\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 11: PLOT WHOLE DATESET**  \n",
    "  \n",
    "**Let us try to visualize all the data at once**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 12: REARRANGING DATA**  \n",
    "  \n",
    "**Given that the data we are using is structured differently. It would make sense to group data by the state and month.**\n",
    "**Therefore we create 2 new dataframes.**  \n",
    "The second i simply an aveage of all the columns in the first one.  \n",
    "**This makes it easier to visualize and analyze the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data.groupby([\"state\",\"month\"], sort=False)\n",
    "new_data= grouped['data_value'].agg(np.mean)   \n",
    "new_data = new_data.unstack(level=-1)\n",
    "display(HTML(new_data.head().to_html()))\n",
    "\n",
    "col = new_data.loc[: , \"January\":\"December\"]\n",
    "new_stateavg_data=pd.DataFrame()\n",
    "new_stateavg_data['All_Month_Average'] =  col.mean(axis=1)\n",
    "new_stateavg_data = new_stateavg_data.drop(['US'])\n",
    "display(HTML(new_stateavg_data.head().to_html()))\n",
    "display(HTML(new_stateavg_data.describe().to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 13: HEATMAP OF CORRELATION BETWEEN COLUMNS**  \n",
    "  \n",
    "**When two sets of data are strongly linked together we say they have a High Correlation. To see corr between all the columns, we use .corr()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(15, 13))\n",
    "sns.heatmap(new_data.corr(), annot=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 14: COMPLEX VISUALIZATION**  \n",
    "  \n",
    "**Next we want to be able to make more complex visualizations to better understand code** \n",
    "  \n",
    "**To start with, lets use plotly.graph_objs to visualize data on a USA map.**  \n",
    "   \n",
    "This can be repeated for the other dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = dict(\n",
    "        type='choropleth',\n",
    "        locations = list(new_stateavg_data.index),\n",
    "        z = new_stateavg_data.All_Month_Average,\n",
    "        locationmode = 'USA-states',\n",
    "        text = list(new_stateavg_data.index),colorbar = {'title':'Total Deaths', 'tickmode' : 'array',},\n",
    "    )\n",
    "\n",
    "layout = dict(title='Location Projections',\n",
    "    geo = dict(projection={'type':'albers usa'}))\n",
    "    \n",
    "fig = dict( data=[data2], layout=layout )\n",
    "iplot( fig )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
